#!/bin/sh

# ASPHub Backup Script - Solution 3-2-1 avec archive S3 des donnÃ©es sources
# Backup local avec Plakar + archive tar.gz des donnÃ©es sources sur S3

set -e

# Configuration
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="/backup/storage/backup.log"
BACKUP_ID="backup_${TIMESTAMP}"
DUMP_DIR="/backup/dumps"
TEMP_DIR="/tmp"

# Repositories (3-2-1 Strategy)
LOCAL_REPO="/backup/storage/local"
S3_BUCKET="{{ vault_backup_s3_bucket }}"
S3_ARCHIVE_NAME="asphub_backup_${TIMESTAMP}.tar.gz"

# Passphrases
LOCAL_PASSPHRASE="{{ vault_backup_local_passphrase }}"

# Fonction de log
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [BACKUP] $1" | tee -a "${LOG_FILE}"
}

error_exit() {
    log "ERROR: $1"
    exit 1
}

# Fonction pour crÃ©er un dump PostgreSQL
create_postgres_dump() {
    log "ğŸ—„ï¸ Creating PostgreSQL dump..."
    
    # CrÃ©er le rÃ©pertoire de dumps
    mkdir -p "$DUMP_DIR"
    
    # Dump PostgreSQL depuis le conteneur
    POSTGRES_CONTAINER=$(docker ps -q -f name=asphub_database_postgres)
    if [ -z "$POSTGRES_CONTAINER" ]; then
        log "WARNING: PostgreSQL container not found, skipping dump"
        return 1
    fi
    
    # CrÃ©er le dump
    docker exec "$POSTGRES_CONTAINER" pg_dump -U postgres asphub > "$DUMP_DIR/postgres_dump_${TIMESTAMP}.sql"
    
    if [ $? -eq 0 ]; then
        log "âœ… PostgreSQL dump created: postgres_dump_${TIMESTAMP}.sql"
        return 0
    else
        log "âŒ PostgreSQL dump failed"
        return 1
    fi
}

# Fonction de backup Plakar local
plakar_backup() {
    local repo="$1"
    local passphrase="$2"
    local description="$3"
    
    log "Creating backup to $repo: $description"
    
    export PLAKAR_REPOSITORY_PASSPHRASE="$passphrase"
    
    # Initialiser le repository s'il n'existe pas
    if ! plakar -no-agent at "$repo" info >/dev/null 2>&1; then
        log "Initializing repository: $repo"
        plakar -no-agent at "$repo" create
    fi
    
    # CrÃ©er le backup avec les sources ET les dumps
    if plakar -no-agent at "$repo" backup /backup/source /backup/dumps; then
        log "âœ… Backup successful: $repo"
        return 0
    else
        log "âŒ Backup failed: $repo"
        return 1
    fi
}

# Fonction pour crÃ©er et uploader l'archive S3
create_s3_archive() {
    log "â˜ï¸ Creating S3 archive with source data..."
    
    # CrÃ©er l'archive tar.gz avec les donnÃ©es sources
    ARCHIVE_PATH="$TEMP_DIR/$S3_ARCHIVE_NAME"
    
    # CrÃ©er l'archive avec les donnÃ©es sources et les dumps
    if tar -czf "$ARCHIVE_PATH" -C /backup source dumps; then
        log "âœ… Archive created: $ARCHIVE_PATH"
        
        # Calculer la taille de l'archive
        ARCHIVE_SIZE=$(du -sh "$ARCHIVE_PATH" | cut -f1)
        log "ğŸ“¦ Archive size: $ARCHIVE_SIZE"
        
        # Uploader sur S3
        log "â¬†ï¸ Uploading to S3: s3://$S3_BUCKET/$S3_ARCHIVE_NAME"
        if aws s3 cp "$ARCHIVE_PATH" "s3://$S3_BUCKET/$S3_ARCHIVE_NAME"; then
            log "âœ… S3 upload successful"
            
            # Nettoyer l'archive temporaire
            rm -f "$ARCHIVE_PATH"
            log "ğŸ§¹ Temporary archive cleaned up"
            
            return 0
        else
            log "âŒ S3 upload failed"
            rm -f "$ARCHIVE_PATH"
            return 1
        fi
    else
        log "âŒ Archive creation failed"
        return 1
    fi
}

# Fonction de nettoyage
cleanup_old_backups() {
    local repo="$1"
    local passphrase="$2"
    local retention_days="{{ backup_retention_days }}"
    
    log "Cleaning up old backups in $repo (keeping last $retention_days days)"
    
    export PLAKAR_REPOSITORY_PASSPHRASE="$passphrase"
    
    # Supprimer les anciens dumps locaux
    find "$DUMP_DIR" -name "*.sql" -mtime +$retention_days -delete 2>/dev/null || true
    
    # Nettoyer les anciennes archives S3 (optionnel - peut Ãªtre gÃ©rÃ© par lifecycle policy)
    log "Note: S3 cleanup can be managed by bucket lifecycle policies"
    
    log "âœ… Cleanup completed"
}

# === MAIN EXECUTION ===

log "ğŸš€ Starting ASPHub 3-2-1 backup (ID: $BACKUP_ID)"

# VÃ©rifier que les donnÃ©es sources existent
if [ ! -d "/backup/source/postgres" ] || [ ! -d "/backup/source/minio" ]; then
    error_exit "Source data not found. Check volume mounts."
fi

# Ã‰TAPE 0: CrÃ©er les dumps PostgreSQL
log "ğŸ—„ï¸ Step 0/4: Creating database dumps"
if create_postgres_dump; then
    log "âœ… PostgreSQL dump created successfully"
else
    log "âš ï¸ PostgreSQL dump failed - continuing with volume backup only"
fi

# Ã‰TAPE 1: Backup local avec Plakar (copie #1)
log "ğŸ“¦ Step 1/4: Creating local backup with Plakar"
if ! plakar_backup "$LOCAL_REPO" "$LOCAL_PASSPHRASE" "ASPHub local backup"; then
    error_exit "Local backup failed"
fi

# Ã‰TAPE 2: CrÃ©er et uploader l'archive S3 (copie #2 - offsite)
log "â˜ï¸ Step 2/4: Creating and uploading S3 archive"
if ! create_s3_archive; then
    log "WARNING: S3 archive creation/upload failed - continuing with backup"
fi

# Ã‰TAPE 3: Nettoyage des anciens backups
log "ğŸ§¹ Step 3/4: Cleaning up old backups"
cleanup_old_backups "$LOCAL_REPO" "$LOCAL_PASSPHRASE"

# Ã‰TAPE 4: VÃ©rification basique du backup local
log "âœ… Step 4/4: Verifying backup integrity"
export PLAKAR_REPOSITORY_PASSPHRASE="$LOCAL_PASSPHRASE"
if plakar -no-agent at "$LOCAL_REPO" check; then
    log "âœ… Backup verification successful"
else
    log "âŒ Backup verification failed"
fi

# Statistiques finales
BACKUP_SIZE=$(du -sh /backup/storage | cut -f1)
POSTGRES_SIZE=$(du -sh /backup/source/postgres | cut -f1)
MINIO_SIZE=$(du -sh /backup/source/minio | cut -f1)
DUMPS_SIZE=$(du -sh /backup/dumps 2>/dev/null | cut -f1 || echo "0")

log "ğŸ“Š Backup Statistics:"
log "   - PostgreSQL data: $POSTGRES_SIZE"
log "   - MinIO data: $MINIO_SIZE"
log "   - SQL dumps: $DUMPS_SIZE"
log "   - Total backup size: $BACKUP_SIZE"
log "   - Backup ID: $BACKUP_ID"
log "   - S3 Archive: $S3_ARCHIVE_NAME"

log "ğŸ‰ Backup completed successfully!"

# CrÃ©er un fichier de statut pour monitoring
cat > /backup/storage/last_backup_status.json << EOF
{
    "backup_id": "$BACKUP_ID",
    "timestamp": "$(date -Iseconds)",
    "status": "success",
    "postgres_size": "$POSTGRES_SIZE",
    "minio_size": "$MINIO_SIZE",
    "dumps_size": "$DUMPS_SIZE",
    "total_size": "$BACKUP_SIZE",
    "s3_archive": "$S3_ARCHIVE_NAME"
}
EOF 