#!/bin/sh

# ASPHub Backup Script - Solution 3-2-1 avec archive S3 des données sources
# Backup local avec Plakar + archive tar.gz des données sources sur S3

set -e

# Configuration
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
LOG_FILE="/backup/storage/backup.log"
BACKUP_ID="backup_${TIMESTAMP}"
DUMP_DIR="/backup/dumps"
TEMP_DIR="/tmp"

# Repositories (3-2-1 Strategy)
LOCAL_REPO="/backup/storage/local"
S3_BUCKET="{{ vault_backup_s3_bucket }}"
S3_ARCHIVE_NAME="asphub_backup_${TIMESTAMP}.tar.gz"

# Passphrases
LOCAL_PASSPHRASE="{{ vault_backup_local_passphrase }}"

# Fonction de log
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [BACKUP] $1" | tee -a "${LOG_FILE}"
}

error_exit() {
    log "ERROR: $1"
    exit 1
}

# Fonction pour créer un dump PostgreSQL
create_postgres_dump() {
    log "🗄️ Creating PostgreSQL dump..."
    
    # Créer le répertoire de dumps
    mkdir -p "$DUMP_DIR"
    
    # Dump PostgreSQL depuis le conteneur
    POSTGRES_CONTAINER=$(docker ps -q -f name=asphub_database_postgres)
    if [ -z "$POSTGRES_CONTAINER" ]; then
        log "WARNING: PostgreSQL container not found, skipping dump"
        return 1
    fi
    
    # Créer le dump
    docker exec "$POSTGRES_CONTAINER" pg_dump -U postgres asphub > "$DUMP_DIR/postgres_dump_${TIMESTAMP}.sql"
    
    if [ $? -eq 0 ]; then
        log "✅ PostgreSQL dump created: postgres_dump_${TIMESTAMP}.sql"
        return 0
    else
        log "❌ PostgreSQL dump failed"
        return 1
    fi
}

# Fonction de backup Plakar local
plakar_backup() {
    local repo="$1"
    local passphrase="$2"
    local description="$3"
    
    log "Creating backup to $repo: $description"
    
    export PLAKAR_REPOSITORY_PASSPHRASE="$passphrase"
    
    # Initialiser le repository s'il n'existe pas
    if ! plakar -no-agent at "$repo" info >/dev/null 2>&1; then
        log "Initializing repository: $repo"
        plakar -no-agent at "$repo" create
    fi
    
    # Créer le backup avec les sources ET les dumps
    if plakar -no-agent at "$repo" backup /backup/source /backup/dumps; then
        log "✅ Backup successful: $repo"
        return 0
    else
        log "❌ Backup failed: $repo"
        return 1
    fi
}

# Fonction pour créer et uploader l'archive S3
create_s3_archive() {
    log "☁️ Creating S3 archive with source data..."
    
    # Créer l'archive tar.gz avec les données sources
    ARCHIVE_PATH="$TEMP_DIR/$S3_ARCHIVE_NAME"
    
    # Créer l'archive avec les données sources et les dumps
    if tar -czf "$ARCHIVE_PATH" -C /backup source dumps; then
        log "✅ Archive created: $ARCHIVE_PATH"
        
        # Calculer la taille de l'archive
        ARCHIVE_SIZE=$(du -sh "$ARCHIVE_PATH" | cut -f1)
        log "📦 Archive size: $ARCHIVE_SIZE"
        
        # Uploader sur S3
        log "⬆️ Uploading to S3: s3://$S3_BUCKET/$S3_ARCHIVE_NAME"
        if aws s3 cp "$ARCHIVE_PATH" "s3://$S3_BUCKET/$S3_ARCHIVE_NAME"; then
            log "✅ S3 upload successful"
            
            # Nettoyer l'archive temporaire
            rm -f "$ARCHIVE_PATH"
            log "🧹 Temporary archive cleaned up"
            
            return 0
        else
            log "❌ S3 upload failed"
            rm -f "$ARCHIVE_PATH"
            return 1
        fi
    else
        log "❌ Archive creation failed"
        return 1
    fi
}

# Fonction de nettoyage
cleanup_old_backups() {
    local repo="$1"
    local passphrase="$2"
    local retention_days="{{ backup_retention_days }}"
    
    log "Cleaning up old backups in $repo (keeping last $retention_days days)"
    
    export PLAKAR_REPOSITORY_PASSPHRASE="$passphrase"
    
    # Supprimer les anciens dumps locaux
    find "$DUMP_DIR" -name "*.sql" -mtime +$retention_days -delete 2>/dev/null || true
    
    # Nettoyer les anciennes archives S3 (optionnel - peut être géré par lifecycle policy)
    log "Note: S3 cleanup can be managed by bucket lifecycle policies"
    
    log "✅ Cleanup completed"
}

# === MAIN EXECUTION ===

log "🚀 Starting ASPHub 3-2-1 backup (ID: $BACKUP_ID)"

# Vérifier que les données sources existent
if [ ! -d "/backup/source/postgres" ] || [ ! -d "/backup/source/minio" ]; then
    error_exit "Source data not found. Check volume mounts."
fi

# ÉTAPE 0: Créer les dumps PostgreSQL
log "🗄️ Step 0/4: Creating database dumps"
if create_postgres_dump; then
    log "✅ PostgreSQL dump created successfully"
else
    log "⚠️ PostgreSQL dump failed - continuing with volume backup only"
fi

# ÉTAPE 1: Backup local avec Plakar (copie #1)
log "📦 Step 1/4: Creating local backup with Plakar"
if ! plakar_backup "$LOCAL_REPO" "$LOCAL_PASSPHRASE" "ASPHub local backup"; then
    error_exit "Local backup failed"
fi

# ÉTAPE 2: Créer et uploader l'archive S3 (copie #2 - offsite)
log "☁️ Step 2/4: Creating and uploading S3 archive"
if ! create_s3_archive; then
    log "WARNING: S3 archive creation/upload failed - continuing with backup"
fi

# ÉTAPE 3: Nettoyage des anciens backups
log "🧹 Step 3/4: Cleaning up old backups"
cleanup_old_backups "$LOCAL_REPO" "$LOCAL_PASSPHRASE"

# ÉTAPE 4: Vérification basique du backup local
log "✅ Step 4/4: Verifying backup integrity"
export PLAKAR_REPOSITORY_PASSPHRASE="$LOCAL_PASSPHRASE"
if plakar -no-agent at "$LOCAL_REPO" check; then
    log "✅ Backup verification successful"
else
    log "❌ Backup verification failed"
fi

# Statistiques finales
BACKUP_SIZE=$(du -sh /backup/storage | cut -f1)
POSTGRES_SIZE=$(du -sh /backup/source/postgres | cut -f1)
MINIO_SIZE=$(du -sh /backup/source/minio | cut -f1)
DUMPS_SIZE=$(du -sh /backup/dumps 2>/dev/null | cut -f1 || echo "0")

log "📊 Backup Statistics:"
log "   - PostgreSQL data: $POSTGRES_SIZE"
log "   - MinIO data: $MINIO_SIZE"
log "   - SQL dumps: $DUMPS_SIZE"
log "   - Total backup size: $BACKUP_SIZE"
log "   - Backup ID: $BACKUP_ID"
log "   - S3 Archive: $S3_ARCHIVE_NAME"

log "🎉 Backup completed successfully!"

# Créer un fichier de statut pour monitoring
cat > /backup/storage/last_backup_status.json << EOF
{
    "backup_id": "$BACKUP_ID",
    "timestamp": "$(date -Iseconds)",
    "status": "success",
    "postgres_size": "$POSTGRES_SIZE",
    "minio_size": "$MINIO_SIZE",
    "dumps_size": "$DUMPS_SIZE",
    "total_size": "$BACKUP_SIZE",
    "s3_archive": "$S3_ARCHIVE_NAME"
}
EOF 